 ## Dataset: BIWI
 ## Attention: LA
 ## Re-ID Manner: ap
 ## Sequence Length: 10
 ## Tempearture: 0.1
 ## Pretext Task: rev_rec
 ## GPU: 0

 ## Training Gait Encoding Model: True
 ## Training Recognition Network: True

wsx log: enter train() folder_name dim train_dataset ./Models/Gait_Encoding_models/BIWI_LA_x_10_0.1_0pre_rev_rec 	 x 	 BIWI
wsx log: get_data_BIWI(dimension, fr) dim x 	 fr 	 10
Datasets/BIWI/10/BIWI_train_npy_data/source_x_BIWI_10.npy
wsx log: frames_ps BIWI/10/
wsx log BIWI input_data shape (68400, 20)
wsx log BIWI input_data reshape (6840, 10, 20)
log: input_data[:3,:,:]->    
 [[[ 0.0413  0.0425  0.038   0.0357 -0.1279 -0.1727 -0.1798 -0.1822
    0.2007  0.2677  0.2896  0.2892 -0.0322 -0.0014  0.0173  0.0161
    0.1158  0.0784  0.0616  0.0572]
  [ 0.034   0.035   0.039   0.033  -0.1263 -0.1731 -0.1743 -0.1708
    0.2015  0.2639  0.2901  0.2918 -0.0424  0.0336  0.0507  0.052
    0.1085  0.1303  0.1145  0.0976]
  [ 0.0347  0.0349  0.0342  0.025  -0.1281 -0.1683 -0.1791 -0.1791
    0.1976  0.2658  0.2905  0.2985 -0.0402  0.03    0.0541  0.0693
    0.11    0.1478  0.1109  0.0917]
  [ 0.0431  0.0422  0.0289  0.0108 -0.1325 -0.1693 -0.1734 -0.173
    0.1952  0.2675  0.2945  0.2987 -0.0302  0.0388  0.0546  0.0672
    0.1211  0.1409  0.1284  0.0904]
  [ 0.0497  0.0504  0.032  -0.0049 -0.1379 -0.1676 -0.1728 -0.1665
    0.196   0.2697  0.2941  0.3042 -0.0231  0.0506  0.0531  0.0728
    0.1288  0.1681  0.164   0.1243]
  [ 0.0383  0.0468  0.0351 -0.0207 -0.1378 -0.1611 -0.1724 -0.1652
    0.1882  0.2651  0.2909  0.2983 -0.0343  0.0313  0.0558  0.0435
    0.1121  0.158   0.1664  0.2207]
  [ 0.029   0.0466  0.0353 -0.0357 -0.1347 -0.1588 -0.1757 -0.1714
    0.1783  0.2685  0.2817  0.2808 -0.0422  0.016   0.0696  0.0428
    0.0976  0.1432  0.2347  0.2218]
  [ 0.0227  0.0491  0.0351 -0.0468 -0.12   -0.1548 -0.1784 -0.1793
    0.1661  0.2628  0.2544  0.2535 -0.0448  0.0077  0.0748  0.0421
    0.0858  0.1313  0.2441  0.2178]
  [ 0.0181  0.0504  0.0318 -0.0567 -0.0846 -0.141  -0.1815 -0.1975
    0.1484  0.2487  0.2277  0.2175 -0.0407  0.0023  0.0751  0.0442
    0.0705  0.1258  0.2383  0.1995]
  [ 0.0053  0.0441  0.0237 -0.0643 -0.0714 -0.1064 -0.1585 -0.1852
    0.1214  0.2063  0.1765  0.174  -0.0401  0.0057  0.0774  0.0494
    0.0445  0.127   0.2385  0.1852]]

 [[-0.1842 -0.1938 -0.1905 -0.1796 -0.3136 -0.3771 -0.4037 -0.4075
   -0.0439  0.0636  0.1172  0.1169 -0.2402 -0.1113 -0.1413 -0.128
   -0.126  -0.2032 -0.2909 -0.2332]
  [-0.1846 -0.1939 -0.2015 -0.2315 -0.3395 -0.3903 -0.4232 -0.4164
   -0.0447  0.0586  0.1046  0.1155 -0.2551 -0.1169 -0.1372 -0.1274
   -0.1091 -0.2468 -0.3088 -0.2767]
  [-0.2054 -0.2184 -0.2265 -0.2263 -0.3445 -0.4057 -0.4418 -0.4408
   -0.0859  0.0163  0.0675  0.0921 -0.2666 -0.1125 -0.1363 -0.1002
   -0.1384 -0.2356 -0.2932 -0.2644]
  [-0.2181 -0.2291 -0.2418 -0.266  -0.3293 -0.4211 -0.4391 -0.4592
   -0.1699  0.0089  0.0554  0.0709 -0.26   -0.1142 -0.119  -0.1068
   -0.1759 -0.2454 -0.2339 -0.2111]
  [-0.2636 -0.2805 -0.274  -0.2913 -0.3774 -0.4409 -0.4698 -0.495
   -0.2676 -0.0617 -0.114  -0.1344 -0.2945 -0.1509 -0.1242 -0.0952
   -0.2312 -0.286  -0.2745 -0.2347]
  [-0.2344 -0.2652 -0.3034 -0.3207 -0.4072 -0.4707 -0.4997 -0.5257
   -0.297  -0.0918 -0.2488 -0.3011 -0.2832 -0.1758 -0.0916 -0.0448
   -0.1591 -0.2779 -0.2664 -0.2016]
  [-0.2344 -0.2652 -0.3034 -0.3207 -0.4072 -0.4707 -0.4997 -0.5257
   -0.297  -0.0918 -0.2488 -0.3011 -0.2832 -0.1758 -0.0916 -0.0448
   -0.1591 -0.2779 -0.2664 -0.2016]
  [-0.0363 -0.0418 -0.0507 -0.0548 -0.1939 -0.2578 -0.2553 -0.2545
    0.1051  0.2008  0.2158  0.2176 -0.1017 -0.0892 -0.0692 -0.0685
    0.0364 -0.1148 -0.0642 -0.0748]
  [-0.0467 -0.0474 -0.0536 -0.0634 -0.2034 -0.2625 -0.286  -0.2744
    0.1053  0.1944  0.2161  0.2177 -0.1154 -0.0861 -0.0576 -0.0636
    0.024  -0.1217 -0.0592 -0.0542]
  [-0.0544 -0.0575 -0.0651 -0.0804 -0.2257 -0.2709 -0.2802 -0.268
    0.1055  0.1855  0.2112  0.2162 -0.1247 -0.0949 -0.0563 -0.0618
    0.0197  0.005  -0.0545 -0.0573]]

 [[-0.275  -0.2512 -0.2522 -0.2956 -0.4201 -0.4718 -0.5087 -0.5185
   -0.1221 -0.0481 -0.039  -0.0472 -0.3462 -0.3421 -0.3164 -0.3485
   -0.2115 -0.1515 -0.0421 -0.0473]
  [-0.3097 -0.2898 -0.2954 -0.3426 -0.4605 -0.5374 -0.5648 -0.5912
   -0.1617 -0.0838 -0.0736 -0.0778 -0.3777 -0.3809 -0.306  -0.3179
   -0.2462 -0.2197 -0.0928 -0.0739]
  [-0.3457 -0.3273 -0.3302 -0.3708 -0.5023 -0.5714 -0.6339 -0.6627
   -0.1932 -0.1213 -0.1135 -0.1195 -0.4183 -0.4283 -0.3062 -0.2951
   -0.2783 -0.2831 -0.2008 -0.1991]
  [-0.3718 -0.3548 -0.3634 -0.401  -0.5324 -0.6022 -0.6799 -0.7192
   -0.2249 -0.1536 -0.147  -0.1653 -0.4443 -0.4843 -0.3676 -0.3234
   -0.302  -0.3144 -0.2455 -0.2519]
  [-0.431  -0.411  -0.4171 -0.4534 -0.5872 -0.6532 -0.7228 -0.7538
   -0.2853 -0.2157 -0.2225 -0.2334 -0.5039 -0.5419 -0.3703 -0.3798
   -0.3626 -0.378  -0.5191 -0.6005]
  [-0.4528 -0.4345 -0.4372 -0.4761 -0.5945 -0.6724 -0.7176 -0.7307
   -0.3076 -0.2452 -0.2538 -0.2649 -0.524  -0.5841 -0.4026 -0.4101
   -0.3864 -0.4342 -0.4936 -0.5416]
  [-0.4752 -0.456  -0.4512 -0.4902 -0.6115 -0.6107 -0.7053 -0.7349
   -0.3261 -0.2658 -0.2767 -0.282  -0.5458 -0.6025 -0.4062 -0.4182
   -0.4114 -0.4465 -0.4801 -0.5117]
  [-0.5398 -0.5168 -0.4942 -0.5326 -0.6413 -0.6613 -0.7594 -0.7783
   -0.3831 -0.3226 -0.3417 -0.3586 -0.606  -0.6695 -0.4837 -0.4424
   -0.4846 -0.4971 -0.51   -0.5416]
  [-0.5398 -0.5168 -0.4942 -0.5326 -0.6413 -0.6613 -0.7594 -0.7783
   -0.3831 -0.3226 -0.3417 -0.3586 -0.606  -0.6695 -0.4837 -0.4424
   -0.4846 -0.4971 -0.51   -0.5416]
  [-0.5895 -0.5668 -0.5547 -0.5935 -0.6856 -0.7056 -0.7123 -0.7184
   -0.4334 -0.3748 -0.4394 -0.4622 -0.6466 -0.6912 -0.7255 -0.7633
   -0.5457 -0.4843 -0.4203 -0.4434]]]
log: enter rev_rec
log: targets shape (68400, 20)
log: targets reshape (6840, 10, 20)
log: targets[:3,:,:]->    
 [[[ 0.0053  0.0441  0.0237 -0.0643 -0.0714 -0.1064 -0.1585 -0.1852
    0.1214  0.2063  0.1765  0.174  -0.0401  0.0057  0.0774  0.0494
    0.0445  0.127   0.2385  0.1852]
  [ 0.0181  0.0504  0.0318 -0.0567 -0.0846 -0.141  -0.1815 -0.1975
    0.1484  0.2487  0.2277  0.2175 -0.0407  0.0023  0.0751  0.0442
    0.0705  0.1258  0.2383  0.1995]
  [ 0.0227  0.0491  0.0351 -0.0468 -0.12   -0.1548 -0.1784 -0.1793
    0.1661  0.2628  0.2544  0.2535 -0.0448  0.0077  0.0748  0.0421
    0.0858  0.1313  0.2441  0.2178]
  [ 0.029   0.0466  0.0353 -0.0357 -0.1347 -0.1588 -0.1757 -0.1714
    0.1783  0.2685  0.2817  0.2808 -0.0422  0.016   0.0696  0.0428
    0.0976  0.1432  0.2347  0.2218]
  [ 0.0383  0.0468  0.0351 -0.0207 -0.1378 -0.1611 -0.1724 -0.1652
    0.1882  0.2651  0.2909  0.2983 -0.0343  0.0313  0.0558  0.0435
    0.1121  0.158   0.1664  0.2207]
  [ 0.0497  0.0504  0.032  -0.0049 -0.1379 -0.1676 -0.1728 -0.1665
    0.196   0.2697  0.2941  0.3042 -0.0231  0.0506  0.0531  0.0728
    0.1288  0.1681  0.164   0.1243]
  [ 0.0431  0.0422  0.0289  0.0108 -0.1325 -0.1693 -0.1734 -0.173
    0.1952  0.2675  0.2945  0.2987 -0.0302  0.0388  0.0546  0.0672
    0.1211  0.1409  0.1284  0.0904]
  [ 0.0347  0.0349  0.0342  0.025  -0.1281 -0.1683 -0.1791 -0.1791
    0.1976  0.2658  0.2905  0.2985 -0.0402  0.03    0.0541  0.0693
    0.11    0.1478  0.1109  0.0917]
  [ 0.034   0.035   0.039   0.033  -0.1263 -0.1731 -0.1743 -0.1708
    0.2015  0.2639  0.2901  0.2918 -0.0424  0.0336  0.0507  0.052
    0.1085  0.1303  0.1145  0.0976]
  [ 0.0413  0.0425  0.038   0.0357 -0.1279 -0.1727 -0.1798 -0.1822
    0.2007  0.2677  0.2896  0.2892 -0.0322 -0.0014  0.0173  0.0161
    0.1158  0.0784  0.0616  0.0572]]

 [[-0.0544 -0.0575 -0.0651 -0.0804 -0.2257 -0.2709 -0.2802 -0.268
    0.1055  0.1855  0.2112  0.2162 -0.1247 -0.0949 -0.0563 -0.0618
    0.0197  0.005  -0.0545 -0.0573]
  [-0.0467 -0.0474 -0.0536 -0.0634 -0.2034 -0.2625 -0.286  -0.2744
    0.1053  0.1944  0.2161  0.2177 -0.1154 -0.0861 -0.0576 -0.0636
    0.024  -0.1217 -0.0592 -0.0542]
  [-0.0363 -0.0418 -0.0507 -0.0548 -0.1939 -0.2578 -0.2553 -0.2545
    0.1051  0.2008  0.2158  0.2176 -0.1017 -0.0892 -0.0692 -0.0685
    0.0364 -0.1148 -0.0642 -0.0748]
  [-0.2344 -0.2652 -0.3034 -0.3207 -0.4072 -0.4707 -0.4997 -0.5257
   -0.297  -0.0918 -0.2488 -0.3011 -0.2832 -0.1758 -0.0916 -0.0448
   -0.1591 -0.2779 -0.2664 -0.2016]
  [-0.2344 -0.2652 -0.3034 -0.3207 -0.4072 -0.4707 -0.4997 -0.5257
   -0.297  -0.0918 -0.2488 -0.3011 -0.2832 -0.1758 -0.0916 -0.0448
   -0.1591 -0.2779 -0.2664 -0.2016]
  [-0.2636 -0.2805 -0.274  -0.2913 -0.3774 -0.4409 -0.4698 -0.495
   -0.2676 -0.0617 -0.114  -0.1344 -0.2945 -0.1509 -0.1242 -0.0952
   -0.2312 -0.286  -0.2745 -0.2347]
  [-0.2181 -0.2291 -0.2418 -0.266  -0.3293 -0.4211 -0.4391 -0.4592
   -0.1699  0.0089  0.0554  0.0709 -0.26   -0.1142 -0.119  -0.1068
   -0.1759 -0.2454 -0.2339 -0.2111]
  [-0.2054 -0.2184 -0.2265 -0.2263 -0.3445 -0.4057 -0.4418 -0.4408
   -0.0859  0.0163  0.0675  0.0921 -0.2666 -0.1125 -0.1363 -0.1002
   -0.1384 -0.2356 -0.2932 -0.2644]
  [-0.1846 -0.1939 -0.2015 -0.2315 -0.3395 -0.3903 -0.4232 -0.4164
   -0.0447  0.0586  0.1046  0.1155 -0.2551 -0.1169 -0.1372 -0.1274
   -0.1091 -0.2468 -0.3088 -0.2767]
  [-0.1842 -0.1938 -0.1905 -0.1796 -0.3136 -0.3771 -0.4037 -0.4075
   -0.0439  0.0636  0.1172  0.1169 -0.2402 -0.1113 -0.1413 -0.128
   -0.126  -0.2032 -0.2909 -0.2332]]

 [[-0.5895 -0.5668 -0.5547 -0.5935 -0.6856 -0.7056 -0.7123 -0.7184
   -0.4334 -0.3748 -0.4394 -0.4622 -0.6466 -0.6912 -0.7255 -0.7633
   -0.5457 -0.4843 -0.4203 -0.4434]
  [-0.5398 -0.5168 -0.4942 -0.5326 -0.6413 -0.6613 -0.7594 -0.7783
   -0.3831 -0.3226 -0.3417 -0.3586 -0.606  -0.6695 -0.4837 -0.4424
   -0.4846 -0.4971 -0.51   -0.5416]
  [-0.5398 -0.5168 -0.4942 -0.5326 -0.6413 -0.6613 -0.7594 -0.7783
   -0.3831 -0.3226 -0.3417 -0.3586 -0.606  -0.6695 -0.4837 -0.4424
   -0.4846 -0.4971 -0.51   -0.5416]
  [-0.4752 -0.456  -0.4512 -0.4902 -0.6115 -0.6107 -0.7053 -0.7349
   -0.3261 -0.2658 -0.2767 -0.282  -0.5458 -0.6025 -0.4062 -0.4182
   -0.4114 -0.4465 -0.4801 -0.5117]
  [-0.4528 -0.4345 -0.4372 -0.4761 -0.5945 -0.6724 -0.7176 -0.7307
   -0.3076 -0.2452 -0.2538 -0.2649 -0.524  -0.5841 -0.4026 -0.4101
   -0.3864 -0.4342 -0.4936 -0.5416]
  [-0.431  -0.411  -0.4171 -0.4534 -0.5872 -0.6532 -0.7228 -0.7538
   -0.2853 -0.2157 -0.2225 -0.2334 -0.5039 -0.5419 -0.3703 -0.3798
   -0.3626 -0.378  -0.5191 -0.6005]
  [-0.3718 -0.3548 -0.3634 -0.401  -0.5324 -0.6022 -0.6799 -0.7192
   -0.2249 -0.1536 -0.147  -0.1653 -0.4443 -0.4843 -0.3676 -0.3234
   -0.302  -0.3144 -0.2455 -0.2519]
  [-0.3457 -0.3273 -0.3302 -0.3708 -0.5023 -0.5714 -0.6339 -0.6627
   -0.1932 -0.1213 -0.1135 -0.1195 -0.4183 -0.4283 -0.3062 -0.2951
   -0.2783 -0.2831 -0.2008 -0.1991]
  [-0.3097 -0.2898 -0.2954 -0.3426 -0.4605 -0.5374 -0.5648 -0.5912
   -0.1617 -0.0838 -0.0736 -0.0778 -0.3777 -0.3809 -0.306  -0.3179
   -0.2462 -0.2197 -0.0928 -0.0739]
  [-0.275  -0.2512 -0.2522 -0.2956 -0.4201 -0.4718 -0.5087 -0.5185
   -0.1221 -0.0481 -0.039  -0.0472 -0.3462 -0.3421 -0.3164 -0.3485
   -0.2115 -0.1515 -0.0421 -0.0473]]]
wsx log process_decoder_inut() enter
wsx log data shape (128, 10, 20)
wsx log ending shape (128, 9, 20)
wsx log series_length 20
wsx log decoder_input shape (128, 19, 20)
wsx log traing_loss
wsx log training_decoder_output shape (128, ?, 20)
wsx log targets shape (128, 10, 20)
Begin Training on Dimension [X]
